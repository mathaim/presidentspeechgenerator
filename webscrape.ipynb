{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import dotenv\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lastnames = ['washington', 'adams', 'jefferson', 'madison', \n",
    "             'monroe', 'jqadams', 'jackson', 'vanburen', 'harrison', \n",
    "             'tyler', 'polk', 'taylor', 'fillmore', 'pierce', 'buchanan', \n",
    "             'lincoln', 'johnson', 'grant', 'hayes', 'garfield', 'arthur', \n",
    "             'cleveland', 'harrison', 'cleveland', 'mckinley', 'roosevelt', \n",
    "             'taft', 'wilson', 'harding', 'coolidge', 'hoover', 'fdroosevelt', \n",
    "             'truman', 'eisenhower', 'kennedy', 'johnson', 'nixon', 'ford', \n",
    "             'carter', 'reagan', 'bush', 'clinton', 'gwbush', 'obama', 'trump', \n",
    "             'biden']\n",
    "urls =[]\n",
    "for lastname in lastnames:\n",
    "        # URL of the Genius lyrics page\n",
    "        url= f'https://millercenter.org/president/{lastname}'\n",
    "        urls.append(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_useragent(self):\n",
    "          r=requests.get('https://httpbin.org/user-agent')\n",
    "          useragent = json.loads(r.text)['user-agent']\n",
    "          return useragent\n",
    "    \n",
    "    def make_headers(self,\n",
    "          email='mathaimadelyn@gmail.com'):\n",
    "          headers = {\n",
    "              'User-Agent': self.get_useragent(),\n",
    "              'From': email\n",
    "              }\n",
    "          return headers\n",
    "\n",
    "ws = WebScraper()\n",
    "headers = ws.make_headers()\n",
    "data_list = []\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    fastfacts = html.find('div', class_=\"fast-facts-wrapper\")\n",
    "    stringdata = fastfacts.find_all('div', class_=\"data string-data\")\n",
    "    datedata = fastfacts.find_all('div', class_=\"data date-data\")\n",
    "    numberdata = fastfacts.find_all('div', class_=\"data number-data\")\n",
    "\n",
    "    fullname_div = fastfacts.find('div', class_=\"data fullname-data\")\n",
    "    if fullname_div is not None:\n",
    "        fullname = fullname_div.text.strip().replace('Full Name\\n', '')\n",
    "    else:\n",
    "        fullname = \"Missing\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data['Full Name'] = fullname\n",
    "\n",
    "    for string in stringdata:\n",
    "        text = string.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "    \n",
    "    for number in numberdata:\n",
    "        text = number.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    for date in datedata[0:1]:\n",
    "        text = date.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    for date in datedata[2:3]:\n",
    "        text = date.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "numberdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Retrieves a random user agent from a website\n",
    "    def get_useragent(self):\n",
    "        r = requests.get('https://httpbin.org/user-agent')\n",
    "        useragent = json.loads(r.text)['user-agent']\n",
    "        return useragent\n",
    "\n",
    "    # Creates a dictionary of headers with the user agent and an email address\n",
    "    def make_headers(self, email='mathaimadelyn@gmail.com'):\n",
    "        headers = {\n",
    "            'User-Agent': self.get_useragent(),\n",
    "            'From': email\n",
    "        }\n",
    "        return headers\n",
    "\n",
    "ws = WebScraper()\n",
    "headers = ws.make_headers()\n",
    "data_list = []\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls:\n",
    "    # Send a GET request to the website\n",
    "    page = requests.get(url, headers=headers)\n",
    "    # Parse the HTML response\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    # Find the \"fast-facts-wrapper\" div\n",
    "    fastfacts = html.find('div', class_=\"fast-facts-wrapper\")\n",
    "    # Find all the \"data\" divs with different classes\n",
    "    stringdata = fastfacts.find_all('div', class_=\"data string-data\")\n",
    "    datedata = fastfacts.find_all('div', class_=\"data date-data\")\n",
    "    numberdata = fastfacts.find_all('div', class_=\"data number-data\")\n",
    "\n",
    "    # Find the \"fullname-data\" div and extract the full name\n",
    "    fullname_div = fastfacts.find('div', class_=\"data fullname-data\")\n",
    "    if fullname_div is not None:\n",
    "        fullname = fullname_div.text.strip().replace('Full Name\\n', '')\n",
    "    else:\n",
    "        fullname = \"Missing\"\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    data = {}\n",
    "\n",
    "    # Add the full name to the dictionary\n",
    "    data['Full Name'] = fullname\n",
    "\n",
    "    # Loop through each string data div and extract the data\n",
    "    for string in stringdata:\n",
    "        text = string.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    # Loop through each number data div and extract the data\n",
    "    for number in numberdata:\n",
    "        text = number.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    # Loop through each date data div and extract the data\n",
    "    for date in datedata[0:1]:\n",
    "        text = date.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    for date in datedata[2:3]:\n",
    "        text = date.text.strip()\n",
    "        variable_name, data_value = text.split('\\n', 1)\n",
    "        data[variable_name.strip()] = data_value.strip()\n",
    "\n",
    "    # Add the data to the list\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# List of US Presidents\n",
    "presidents = [\n",
    "    \"George Washington\",\n",
    "    \"John Adams\",\n",
    "    \"Thomas Jefferson\",\n",
    "    \"James Madison\",\n",
    "    \"James Monroe\",\n",
    "    \"John Quincy Adams\",\n",
    "    \"Andrew Jackson\",\n",
    "    \"Martin Van Buren\",\n",
    "    \"William Henry Harrison\",\n",
    "    \"John Tyler\",\n",
    "    \"James K. Polk\",\n",
    "    \"Zachary Taylor\",\n",
    "    \"Millard Fillmore\",\n",
    "    \"Franklin Pierce\",\n",
    "    \"James Buchanan\",\n",
    "    \"Abraham Lincoln\",\n",
    "    \"Andrew Johnson\",\n",
    "    \"Ulysses S. Grant\",\n",
    "    \"Rutherford B. Hayes\",\n",
    "    \"James A. Garfield\",\n",
    "    \"Chester A. Arthur\",\n",
    "    \"Grover Cleveland\",\n",
    "    \"Benjamin Harrison\",\n",
    "    \"Grover Cleveland\",\n",
    "    \"William McKinley\",\n",
    "    \"Theodore Roosevelt\",\n",
    "    \"William Howard Taft\",\n",
    "    \"Woodrow Wilson\",\n",
    "    \"Warren G. Harding\",\n",
    "    \"Calvin Coolidge\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"George H. W. Bush\",\n",
    "    \"Bill Clinton\",\n",
    "    \"George W. Bush\",\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Joe Biden\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "presnames = pd.DataFrame({\n",
    "    \"President Number\": range(1, len(presidents) + 1),\n",
    "    \"Full Name\": presidents,\n",
    "    \"lastname\": lastnames\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "presnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the missing Full Name values\n",
    "\n",
    "for i, data in enumerate(data_list):\n",
    "    lastname = data.get('Full Name')\n",
    "    if lastname == 'Missing':\n",
    "        for j, president in enumerate(presnames):\n",
    "            if j == i:  # assuming you want to match the index of president with the index of data\n",
    "                data_list[i]['Full Name'] = presnames.loc[j, 'Full Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essaylist = ['domestic-affairs','life-before-the-presidency','life-in-brief',\n",
    "             'campaigns-and-elections','foreign-affairs','life-after-the-presidency',\n",
    "             'the-american-franchise','death-of-the-president','impact-and-legacy']\n",
    "essay_urls=[]\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Loop through each president last name\n",
    "for lastname in lastnames:\n",
    "    essays ={}\n",
    "    for essay in essaylist: \n",
    "        # URL of each essay +person\n",
    "        url= f'https://millercenter.org/president/{lastname}/{essay}'\n",
    "        essays[essay]=url\n",
    "        essay_urls.append(url)\n",
    "    data[lastname] = essays\n",
    "\n",
    "# Convert the data to a JSON object\n",
    "essays_json= json.dumps(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "print(yaml.dump(essays_json, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {}\n",
    "for url in essay_urls:\n",
    "    page = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    body = html.find('div', class_=\"article-wysiwyg-body\")\n",
    "\n",
    "    if body is not None:\n",
    "        for tag in body.find_all(['p', 'h4']):\n",
    "            tag.unwrap()\n",
    "\n",
    "        # Get the lastname from the URL\n",
    "        lastname = url.split('/')[-2]\n",
    "        essayname = url.split('/')[-1]\n",
    "\n",
    "        # Create a dictionary structure for each essay\n",
    "        if lastname not in text:\n",
    "            text[lastname] = {}\n",
    "        if essayname not in text[lastname]:\n",
    "            essay_text = body.get_text().strip().replace('\\n', ' ').replace('\\u2019', '').replace('\\u2014', '')\n",
    "            text[lastname][essayname] = essay_text\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error: Could not find body element for URL {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# assume 'text' is your JSON object\n",
    "with open('essays.json', 'w') as f:\n",
    "    json.dump(text, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists for speech titles and links\n",
    "speech_titles = []\n",
    "speech_links = []\n",
    "\n",
    "for i in range(1, 54):  # Loop through all pages\n",
    "    try:\n",
    "        # Define the base URL with pagination\n",
    "        base_url = f\"https://millercenter.org/search?search=speeches&f%5B0%5D=search_facet%3A30871&page={i}\"\n",
    "        \n",
    "        # Get the page content\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all speech titles\n",
    "        views_rows = soup.find_all('div', class_='views-field views-field-title')\n",
    "        speech_titles.extend([row.get_text(strip=True) for row in views_rows])\n",
    "\n",
    "        # Find all speech links\n",
    "        views_rows2 = soup.find_all('div', class_='field-content smallspace-bottom')\n",
    "        links = [row.find('a')['href'] for row in views_rows2 if row.find('a')]\n",
    "        speech_links.extend(links)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL {base_url}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the speech_titles and speech_links lists\n",
    "df = pd.DataFrame({'Speech': speech_titles, 'Link': speech_links})\n",
    "\n",
    "# Split the Speech column into SpeechDate and SpeechTitle\n",
    "df[['SpeechDate', 'SpeechTitle']] = df['Speech'].str.split(':', n=1, expand=True)\n",
    "\n",
    "# Drop the original Speech column\n",
    "df = df.drop('Speech', axis=1)\n",
    "\n",
    "# Split the SpeechDate column into Month, Day, and Year\n",
    "df[['Month', 'Day', 'Year']] = df['SpeechDate'].str.extract(r'(\\w+)\\s+(\\d+),\\s+(\\d+)')\n",
    "\n",
    "# Drop the original SpeechDate column\n",
    "df = df.drop('SpeechDate', axis=1)\n",
    "\n",
    "# Convert Month to lowercase\n",
    "df['Month'] = df['Month'].str.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists for speech titles and links\n",
    "speech_titles = []  # O(1) initialization\n",
    "speech_links = []   # O(1) initialization\n",
    "\n",
    "for i in range(1, 54):  # Loop through all pages; O(53) = O(1) since 53 is constant\n",
    "    try:\n",
    "        # Define the base URL with pagination\n",
    "        base_url = f\"https://millercenter.org/search?search=speeches&f%5B0%5D=search_facet%3A30871&page={i}\"  # O(1)\n",
    "\n",
    "        # Get the page content\n",
    "        response = requests.get(base_url)  # O(1) per request (ignoring network delays)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')  # O(n), where n = size of HTML content\n",
    "\n",
    "        # Find all speech titles\n",
    "        views_rows = soup.find_all('div', class_='views-field views-field-title')  # O(n), searching in HTML\n",
    "        speech_titles.extend([row.get_text(strip=True) for row in views_rows])  # O(k), k = number of titles\n",
    "\n",
    "        # Find all speech links\n",
    "        views_rows2 = soup.find_all('div', class_='field-content smallspace-bottom')  # O(n), searching in HTML\n",
    "        links = [row.find('a')['href'] for row in views_rows2 if row.find('a')]  # O(k), k = number of links\n",
    "        speech_links.extend(links)  # O(k), extending the list\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL {base_url}: {e}\")  # O(1)\n",
    "\n",
    "# Create a DataFrame from the speech_titles and speech_links lists\n",
    "df = pd.DataFrame({'Speech': speech_titles, 'Link': speech_links})  # O(m), m = total speeches\n",
    "\n",
    "# Split the Speech column into SpeechDate and SpeechTitle\n",
    "df[['SpeechDate', 'SpeechTitle']] = df['Speech'].str.split(':', n=1, expand=True)  # O(m * l), l = avg string length\n",
    "\n",
    "# Drop the original Speech column\n",
    "df = df.drop('Speech', axis=1)  # O(1), dropping a column\n",
    "\n",
    "# Split the SpeechDate column into Month, Day, and Year\n",
    "df[['Month', 'Day', 'Year']] = df['SpeechDate'].str.extract(r'(\\w+)\\s+(\\d+),\\s+(\\d+)')  # O(m * r), r = regex complexity\n",
    "\n",
    "# Drop the original SpeechDate column\n",
    "df = df.drop('SpeechDate', axis=1)  # O(1), dropping a column\n",
    "\n",
    "# Convert Month to lowercase\n",
    "df['Month'] = df['Month'].str.lower()  # O(m * l), l = avg length of month strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set pandas display options to show the full content of cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the DataFrame                                      \n",
    "print(df)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_urls = df['Link'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for i, url in enumerate(speech_urls):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    body_wrapper = html.find('div', class_='presidential-speeches--body-wrapper')\n",
    "    speechpresname = body_wrapper.find('label', class_='presidential-speeches--label')\n",
    "    speechpresname = speechpresname.get_text(strip=True).replace(\" Presidency\", \"\")\n",
    "\n",
    "    speech = html.find(['div', 'span'], class_=['view-transcript', 'transcript-inner'])\n",
    "    if speech:\n",
    "        speech_text = speech.get_text(strip=True)\n",
    "        speech_text = speech_text.replace(\"View\", \"\", 1).replace(\"Transcript\", \"\", 2)\n",
    "    else:\n",
    "        speech_text = \"\"\n",
    "\n",
    "    data = {\n",
    "        \"PresidentName\": speechpresname,\n",
    "        \"Speech\": speech_text,\n",
    "        \"url\":url\n",
    "    }\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "df2 = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df2, left_on='Link', right_on='url')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dict = merged_df.to_dict(orient='records')\n",
    "with open('merged_data.json', 'w') as f:\n",
    "    json.dump(merged_df_dict, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Restructure the data\n",
    "restructured_data = {}\n",
    "for speech in merged_df_dict:\n",
    "    president_name = speech['PresidentName'].strip()\n",
    "    if president_name not in restructured_data:\n",
    "        restructured_data[president_name] = []\n",
    "    restructured_data[president_name].append({\n",
    "        'SpeechTitle': speech['SpeechTitle'],\n",
    "        'Month': speech['Month'],\n",
    "        'Day': speech['Day'],\n",
    "        'Year': speech['Year'],\n",
    "        'Speech': speech['Speech'],\n",
    "        'url': speech['url']\n",
    "    })\n",
    "\n",
    "# Print the restructured data\n",
    "print(json.dumps(restructured_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Restructure the data\n",
    "restructured_data = {}\n",
    "for speech in merged_df_dict:\n",
    "    president_name = speech['PresidentName'].strip()\n",
    "    speech_text = speech['Speech']\n",
    "    if president_name not in restructured_data:\n",
    "        restructured_data[president_name] = set()\n",
    "    restructured_data[president_name].add(speech_text)\n",
    "\n",
    "# Convert sets back to lists\n",
    "for president_name in restructured_data:\n",
    "    restructured_data[president_name] = [{'text': speech_text} for speech_text in restructured_data[president_name]]\n",
    "\n",
    "# Print the restructured data\n",
    "print(json.dumps(restructured_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('speechesbypres.json', 'w') as f:\n",
    "    json.dump(restructured_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('speechesbypres.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the count of speeches for each president, ordered by most speeches\n",
    "for president, speeches in sorted(data.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"{president}: {len(speeches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrans_venv_3_12_5_92324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
