{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f759c-1aa8-4ca3-a6a3-a6179ba084bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9349d-12ef-4eb0-b8a7-b6ec0eec27ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "SKIP_QUANTIZATION_MODULES = []\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c18792-7e96-488d-8ba9-918af797c145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871a9c0-4aab-416f-8325-b2fc1833bd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the essays data\n",
    "with open('essays.json', 'r') as file:\n",
    "    essays = json.load(file)\n",
    "\n",
    "# Function to get context text for a given president\n",
    "def get_president_context(pres_name):\n",
    "    # Convert the president's name to lowercase and extract the last name\n",
    "    last_name = pres_name.split()[-1].lower()\n",
    "    print(f\"Looking for: {last_name}\")  # Debug print to check the key\n",
    "    \n",
    "    # Check if the last name exists as a key in the essays data\n",
    "    context = essays.get(last_name, None)\n",
    "    \n",
    "    if context is None:\n",
    "        print(f\"No data found for {last_name}\")  # Debug if key is not found\n",
    "        return \"\"\n",
    "    \n",
    "    # Construct the context text from various sections\n",
    "    context_text = \"\"\n",
    "    for key, value in context.items():\n",
    "        context_text += f\"{key.replace('-', ' ').title()}: {value}\\n\"\n",
    "    \n",
    "    return context_text\n",
    "\n",
    "# Function to handle user input and retrieve context\n",
    "def get_context_from_prompt(prompt):\n",
    "    # Extract the full name from the prompt (assuming last two words are the president's name)\n",
    "    pres_name = ' '.join(prompt.split()[-2:])\n",
    "    \n",
    "    # Retrieve and construct the context text\n",
    "    context_text = get_president_context(pres_name)\n",
    "    \n",
    "    if context_text:\n",
    "        print(\"Generated context for the president:\\n\")\n",
    "        print(context_text)\n",
    "    else:\n",
    "        print(\"No context text generated.\")\n",
    "\n",
    "# Example usage\n",
    "input_prompt = \"Pretend to be Bill Clinton\"\n",
    "print(get_context_from_prompt(\"Pretend to be Bill Clinton\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d5040-57c0-4a20-a12a-75e3c14aff4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the string template for alpaca_prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "dataset = load_dataset('json', data_files='merged_data.json', split='train')\n",
    "\n",
    "# Print dataset columns for verification\n",
    "print(dataset.column_names)\n",
    "\n",
    "\n",
    "\n",
    "# Function to format the dataset\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples['Speech'])):\n",
    "        # Extract relevant fields from the dataset\n",
    "        president_name = examples['PresidentName'][i].strip()\n",
    "        speech_title = examples['SpeechTitle'][i]\n",
    "        speech_content = examples['Speech'][i]\n",
    "\n",
    "        # Generate the input prompt and retrieve context text\n",
    "        input_prompt = f\"Pretend to be {president_name}\"\n",
    "        #context_text = get_president_context(president_name)\n",
    "        \n",
    "        # Construct the full input text with context\n",
    "        input_text = f\"{input_prompt}. Write a lengthy speech on {speech_title}.\"\n",
    "\n",
    "        # Create the instruction and output text\n",
    "        instruction = \"Generate text that simulates how a president would speak based on a given topic. Go based off their mannerisms, opinions, and common themes from context.\"\n",
    "        output_text = f\"{speech_content}\"\n",
    "\n",
    "        # Format the text into the prompt structure and add EOS token\n",
    "        text = alpaca_prompt.format(instruction, input_text, output_text) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Map the function to the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Check a few entries to ensure mapping worked\n",
    "for i in range(1):  # Change 5 to the number of samples you want to check\n",
    "    print(dataset[i]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc38b4-0977-428d-b96b-4944c624f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2778e4-b2ca-4195-8246-291f45dcf49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e00c06-3c79-4b52-b7f1-a29e835764c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b858d34-2e1b-428c-9f40-2a27c047b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fine_tuned_model\"\n",
    "\n",
    "# Save the model and its configuration\n",
    "fine_tuned_model = model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19c69c-396b-46cb-b06b-cf723de64fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Get the model's outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Shift logits and input_ids for next-token prediction\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "    # Compute cross-entropy loss (negative log-likelihood)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # Calculate perplexity\n",
    "    mean_loss = loss.mean().item()\n",
    "    perplexity = torch.exp(torch.tensor(mean_loss)).item()\n",
    "    return perplexity\n",
    "\n",
    "# Example text\n",
    "text = \"My fellow Americans, today I am directing the Department of State to suspend entry of all aliens who are members of the Seventh Day Adventist Church and who were born in Iran, except for those diplomats who are coming here on official business. I have taken this action because we have recently learned of a threat that members of that sect from Iran may be planning acts of violence against Americans. I must act now to protect the safety of our citizens. But I want to assure you that this suspension will be reviewed as conditions change. I have also directed the Secretary of State to suspend all visas for aliens from Iran who are seeking entry into the United States for business, tourist, or temporary visits. However, this suspension will not apply to persons who are traveling for medical treatment, or to meet family members. I have also directed the Secretary of State to suspend the issuance of visas to all Iranian students. These suspensions will be reviewed as conditions change. I am not happy about the prospect of these suspensions. But I must act now to protect the safety of our citizens. And I will review these suspensions as conditions change. Thank you. And God bless America.\"\n",
    "perplexity = calculate_perplexity(text)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ded256-f198-4b87-8f67-1b47119e8569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = \"fine_tuned_model\"\n",
    "\n",
    "# Save the model and its configuration\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25315c79-1701-46d5-a58e-981cad20415a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7571f-9f8a-48df-b3f4-d221be4f9c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# If using PEFT (e.g., LoRA), load the adapter configuration and weights\n",
    "adapter_path = \"fine_tuned_model\"  # Replace with your adapter model path\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef3eb2-915d-4b85-95f1-8e9c7cf4676c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Save the entire model including adapter weights\n",
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba55d3-5600-4007-803e-121001ade4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the base model\n",
    "base_model_path = \"path_to_base_model\"  # e.g., 'llama-model' or any pretrained model path\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "\n",
    "# Load the adapter\n",
    "adapter_path = \"opinionatedllamas\"\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Combine the adapter with the base model to get the full model\n",
    "# This operation may vary depending on the specific PEFT implementation you're using\n",
    "full_model = model.merge_adapter()  # Check your library's documentation for specific methods\n",
    "\n",
    "# Save the full model in the standard format\n",
    "full_model.save_pretrained(\"path_to_save_full_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3165b6f-1a9f-4064-900b-571df8a3fc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")  # Ensure the model is on the GPU if using CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399d271-bfb5-46c3-b180-309e2fd1c737",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ab58-8eef-4e2e-8a37-fb7732d08dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Donald Trump. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72375812-90de-4e1e-ae4d-1613fbe22e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Barack Obama. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a67082-8a8d-4a29-99f2-b429817d0049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Donald Trump. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931e7fe-101d-45a6-90f8-dedb8a6070fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Bill Clinton. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75093699-095d-4073-a721-11b705419170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for your specific model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B\")  # Replace \"gpt2\" with your model identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74a91f-ae58-4768-9c3b-3935f5164bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Donald Trump. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092c36d-aee4-415a-a5a7-18c7a6459bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Barack Obama and use his voice. Tell me your thoughts on the public issue of immigration.\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd4330-fc73-4eb9-97b0-c826db1ed676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Donald Trump. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccc0b5-6e69-4334-b760-c16eb7ace137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.models import FastLanguageModel\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model.config.max_position_embeddings = 2048  # or any appropriate value\n",
    "\n",
    "\n",
    "\n",
    "def generate_president_opinion( president_name, topic):\n",
    "        \"\"\"\n",
    "        Generates a response from the model as if it's written by a specified president on a given topic.\n",
    "        \"\"\"\n",
    "        max_seq_length = model.config.max_position_embeddings \n",
    "        # Define the prompt template\n",
    "        alpaca_prompt = (\n",
    "            \"Provide the response in first person as the president.\\n\"\n",
    "            \"Pretend to be {president_name}. Write a short paragraph on your opinions on {topic}.\"\n",
    "        )\n",
    "\n",
    "        print(\"Generating input tokens...\")\n",
    "        # Create the input text with dynamic substitution for the president and topic\n",
    "        input_text = alpaca_prompt.format(president_name=president_name, topic=topic)\n",
    "        inputs = tokenizer(\n",
    "            [input_text],  # Provide formatted input text\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        print(\"Input tokens generated.\")\n",
    "\n",
    "        # Set up the text streamer for real-time generation\n",
    "        text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        print(\"Generating output...\")\n",
    "        # Generate output using the model\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            streamer=text_streamer,\n",
    "            max_new_tokens=208,  # Adjust as needed\n",
    "            do_sample=True,  # Enables sampling for varied outputs\n",
    "            temperature=0.9,  # Adjust temperature for more creative responses\n",
    "        )\n",
    "        print(\"Output generation completed.\")\n",
    "        \n",
    "        # Decode and print the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf4772-1f38-4307-a049-8cc25ac0f3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "president_name = \"Bill Clinton\"\n",
    "topic = \"immigration\"\n",
    "generate_president_opinion(\"Bill Clinton\", 'immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfd8ab-8a8f-44c2-b6d1-f8b75d243b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person as the president.\",  # instruction\n",
    "        \"Pretend to be Ronald Reagan. Write a short paragraph on your opinions on immigration\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a261544-a67f-46d4-a0ab-da402d1b4cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Function to format the prompt dynamically\n",
    "def generate_president_opinion(president_name, topic):\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Wrap your model for inference\n",
    "    global model  # Ensure model is accessible if already loaded elsewhere\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt_template = (\n",
    "        \"Provide the response in first person as the president.\\n\"\n",
    "        \"Pretend to be {president_name}. Write a short paragraph on your opinions on {topic}.\"\n",
    "    )\n",
    "\n",
    "    # Format the instruction and input dynamically\n",
    "    instruction = \"Provide the response in first person as the president.\"\n",
    "    input_text = f\"Pretend to be {president_name}. Write a short paragraph on your opinions on {topic}.\"\n",
    "    \n",
    "    print(\"Generating input tokens...\")\n",
    "    inputs = tokenizer(\n",
    "        [alpaca_prompt.format(instruction, input_text, \"\")],  # Blank output for generation\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    print(\"Input tokens generated.\")\n",
    "\n",
    "    print(\"Generating output...\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,  # Enables sampling for more diverse outputs\n",
    "        temperature=0.9,  # Adjust temperature for varied output\n",
    "    )\n",
    "    print(\"Output generation completed.\")\n",
    "\n",
    "# Set the president and topic\n",
    "president_name = \"Ronald Reagan\"  # Change to desired president\n",
    "topic = \"immigration\"  # Change to desired topic\n",
    "\n",
    "# Call the function\n",
    "generate_president_opinion(president_name, topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3fad4-319a-47d9-ae60-cd62bd3e520d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person.\",  # instruction\n",
    "        \"Pretend to be Abraham Lincoln and give your thoughts on the Civil War\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fbc0a-3fc1-4136-9641-8f475d6e1efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person.\",  # instruction\n",
    "        \"Pretend to be Joe Biden and give your thoughts on salad\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06e364-6f4d-4e04-aa22-bb693bcf7c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person.\",  # instruction\n",
    "        \"Pretend to be Ronald Reagan and give your thoughts on movies\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22556efb-41b1-4b3d-b34e-7fa2d0ad1b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "from unsloth.models import FastLanguageModel\n",
    "\n",
    "# Wrap your model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "print(\"Generating input tokens...\")\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(\n",
    "        \"Provide the repsonse in first person voice.\",  # instruction\n",
    "        \"Pretend to be Donald Trump and give your thoughts on movies\",  # input\n",
    "        \"\",  # output - leave this blank for generation!\n",
    "    )], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(\"Input tokens generated.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enables sampling for more diverse outputs\n",
    "    temperature=0.9,  # Adjust temperature for varied output\n",
    ")\n",
    "print(\"Output generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b696778-2ddb-4ad0-816a-a6dd4bb643a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93253af9-13cb-4c16-bcf5-b6636050e30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219bc5e-003e-4047-99d5-06d769059cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b5eea-968e-4a9e-9822-50b3dc7542b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_president_opinion(president_name, topic):\n",
    " \n",
    "    \n",
    "    # Create the prompt\n",
    "    alpaca_prompt = (\n",
    "        \"Provide the response in first person as the president.\\n\"\n",
    "        \"Pretend to be {president_name}. Write a short paragraph on your opinions on {topic}.\"\n",
    "    )\n",
    "    input_text = alpaca_prompt.format(president_name=president_name, topic=topic)\n",
    "    \n",
    "    # Generate input tokens\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd50782-24a8-46dd-b982-169c79e37426",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_president_opinion('Bill Clinton', 'economy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d00ab3-2308-421e-bc23-613cdf7ec28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
